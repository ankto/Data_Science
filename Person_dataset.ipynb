{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as datasets \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Подготовка данных для распознавания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1671\n",
      "902\n"
     ]
    }
   ],
   "source": [
    "# Каталог с набором данных \n",
    "data_dir = './TT_Person_dataset/'\n",
    "# Каталог с данными для обучения\n",
    "train_dir = 'trainset'\n",
    "# Каталог с данными для тестирования\n",
    "test_dir = 'testset'\n",
    "# Часть набора данных для тренировки\n",
    "train_data_portion = 0.6\n",
    "# Часть набора данных для тестирования\n",
    "test_data_portion = 0.4\n",
    "# Количество элементов данных в одном классе neg\n",
    "nb_neg_images= len(os.listdir('./TT_Person_dataset/neg/'))\n",
    "print(nb_neg_images)\n",
    "# Количество элементов данных в одном классе pos\n",
    "nb_pos_images = len(os.listdir('./TT_Person_dataset/pos/'))\n",
    "print(nb_pos_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем функцию для каталогов train/test c папками pos/neg в каждой из них\n",
    "def create_directory(dir_name):\n",
    "    if os.path.exists(dir_name):\n",
    "        shutil.rmtree(dir_name)\n",
    "    else:\n",
    "        os.makedirs(os.path.join(data_dir, dir_name, 'pos'))\n",
    "        os.makedirs(os.path.join(data_dir, dir_name, 'neg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем директорию\n",
    "create_directory(train_dir)\n",
    "create_directory(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "541\n",
      "1002\n",
      "361\n",
      "669\n"
     ]
    }
   ],
   "source": [
    "# Расчет индексов наборов данных для обучения и тестирования\n",
    "train_data_pos_idx = int(nb_pos_images * (1-test_data_portion))\n",
    "train_data_neg_idx = int(nb_neg_images * (1-test_data_portion))\n",
    "\n",
    "test_data_pos_idx = int(nb_pos_images * (1-train_data_portion))\n",
    "test_data_neg_idx = int(nb_neg_images * (1-train_data_portion))\n",
    "\n",
    "# train\n",
    "train_data_pos_idx = os.listdir('./TT_Person_dataset/pos/')[:train_data_pos_idx]\n",
    "train_data_neg_idx = os.listdir('./TT_Person_dataset/neg/')[:train_data_neg_idx]\n",
    "\n",
    "#test\n",
    "test_data_pos_idx = os.listdir('./TT_Person_dataset/pos/')[-test_data_pos_idx-1:]\n",
    "test_data_neg_idx = os.listdir('./TT_Person_dataset/neg/')[-test_data_neg_idx-1:]\n",
    "                                                           \n",
    "print(len(train_data_pos_idx))\n",
    "print(len(train_data_neg_idx))\n",
    "print(len(test_data_pos_idx))\n",
    "print(len(test_data_neg_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Распределяем изображения из pos/neg в каталоги train/test в пропорции 60/40\n",
    "def copy_pos_images(data_pos_idx, source_dir, dest_dir):\n",
    "    for i in data_pos_idx:\n",
    "        shutil.copy(os.path.join(source_dir,'pos/', i), os.path.join(dest_dir, 'pos/'))\n",
    "\n",
    "def copy_neg_images(data_neg_idx, source_dir, dest_dir):\n",
    "    for i in data_neg_idx:\n",
    "        shutil.copy(os.path.join(source_dir,'neg/', i), os.path.join(dest_dir, 'neg/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Копирование изображений для train\n",
    "dest_train_dir = './TT_Person_dataset/trainset/'\n",
    "copy_pos_images(train_data_pos_idx, data_dir, dest_train_dir)\n",
    "copy_neg_images(train_data_neg_idx, data_dir, dest_train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Копирование изображений для test\n",
    "dest_test_dir = './TT_Person_dataset/testset/'\n",
    "copy_pos_images(test_data_pos_idx, data_dir, dest_test_dir)\n",
    "copy_neg_images(test_data_neg_idx, data_dir, dest_test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Трансформируем наш датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.Resize(32),\n",
    "    #T.RandomCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "trainset_root = './TT_Person_dataset/trainset/'\n",
    "testset_root = './TT_Person_dataset/testset/'\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(trainset_root, transform = transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle = True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.ImageFolder(testset_root, transform = transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle = False, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n",
      "torch.Size([4, 3, 32, 32])\n",
      "torch.Size([4])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Размерность trainloader/testloader\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "                      \n",
    "x, y = trainset[5]\n",
    "plt.imshow(x.numpy().transpose((1, 2, 0)))\n",
    "print(x.shape)\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Определение модели сверточной нейронной сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
    "        #self.conv3 = nn.Conv2d(32,120, 5)\n",
    "        self.global_pool = nn.MaxPool2d(23, 23)\n",
    "        \n",
    "        self.l1 = nn.Linear(32, 16)\n",
    "        self.l2 = nn.Linear(16, 8)\n",
    "        self.l3 = nn.Linear(8,2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        #print(x.shape)\n",
    "        x = self.pool(x)\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        #print(x.shape)\n",
    "        #x = self.pool(x)\n",
    "        #x = F.relu(self.conv3(x))\n",
    "        x = self.global_pool(x)\n",
    "        #print(x.shape)\n",
    "        x = x.view(-1, 32)\n",
    "        #print(x.shape)\n",
    "       \n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = self.l3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mean(object):\n",
    "    def __init__(self):\n",
    "        self.values = []\n",
    "\n",
    "    def compute(self):\n",
    "        return sum(self.values) / len(self.values)\n",
    "\n",
    "    def update(self, value):\n",
    "        self.values.extend(np.reshape(value, [-1]))\n",
    "\n",
    "    def reset(self):\n",
    "        self.values = []\n",
    "\n",
    "    def compute_and_reset(self):\n",
    "        value = self.compute()\n",
    "        self.reset()\n",
    "\n",
    "        return value\n",
    "\n",
    "def accuracy(a, b):\n",
    "    return (a == b).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Реализация модели и вывод accuracy/loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                   | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "bool value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-9b3ec68d466b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m#model.train()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Функция ошибки\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36mresnet34\u001b[1;34m(pretrained, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m     \"\"\"\n\u001b[0;32m    185\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mResNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBasicBlock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_zoo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_urls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'resnet34'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: bool value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "# Define Model\n",
    "model = Model()\n",
    "\n",
    "#model = torchvision.models.resnet34\n",
    "#model.fc = nn.Linear(512, 2)\n",
    "\n",
    "lr = 1e-3  # learning rate\n",
    "\n",
    "opt = torch.optim.SGD(model.parameters(), lr, momentum=0.9)\n",
    "#opt = torch.optim.Adam(model.parameters(), lr)\n",
    "stats = {\n",
    "    'train_loss': [],\n",
    "    'test_loss': [],\n",
    "    'train_acc': [],\n",
    "    'test_acc': []\n",
    "}\n",
    "\n",
    "# mean_loss = Mean()\n",
    "# mean_acc = Mean()\n",
    "\n",
    "for epoch in tqdm(range(20)):\n",
    "    train_accs = []\n",
    "    train_losses = []\n",
    "    test_accs = []\n",
    "    test_losses = []\n",
    "    \n",
    "    #model.train()\n",
    "    for images, targets in trainloader: \n",
    "        logits = model(images)\n",
    "        \n",
    "        train_loss = F.cross_entropy(input = logits, target = targets) # Функция ошибки\n",
    "        train_acc = accuracy(targets, logits.argmax(1))   \n",
    "        \n",
    "        train_losses.append(train_loss.data.numpy())\n",
    "        train_accs.append(train_acc.data.numpy())\n",
    "\n",
    "        opt.zero_grad()\n",
    "        train_loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    train_loss = np.mean(train_losses) \n",
    "    train_acc = np.mean(train_accs)\n",
    "    \n",
    "    #model.test()\n",
    "    for images, targets in testloader: \n",
    "        logits = model(images)\n",
    "        \n",
    "        test_loss = F.cross_entropy(input = logits, target = targets)\n",
    "        #test_loss = F.binary_cross_entropy(input = logits, target = targets)\n",
    "        test_acc = accuracy(targets, logits.argmax(1))   \n",
    "        \n",
    "        test_losses.append(test_loss.data.numpy())\n",
    "        test_accs.append(test_acc.data.numpy())\n",
    "\n",
    "    test_loss = np.mean(test_losses) \n",
    "    test_acc = np.mean(test_accs)\n",
    "\n",
    "    \n",
    "    print('epoch {}, train loss {:.2f},train acc {:.2f}'.format(epoch, train_loss,train_acc))\n",
    "    stats['train_loss'].append(train_loss)\n",
    "    stats['train_acc'].append(train_acc)\n",
    "    \n",
    "    print('epoch {}, test loss {:.2f},test acc {:.2f}'.format(epoch, test_loss,test_acc))\n",
    "    stats['test_loss'].append(test_loss)\n",
    "    stats['test_acc'].append(test_acc)\n",
    "    \n",
    "    plt.plot(stats['train_loss'], label = 'train')\n",
    "    plt.plot(stats['test_loss'], label = 'test')\n",
    "    plt.title('loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(stats['train_acc'], label = 'train')\n",
    "    plt.plot(stats['test_acc'], label = 'test')\n",
    "    plt.title('acc')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
